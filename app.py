# app.py
import streamlit as st
import os
import platform
import subprocess
from gtts import gTTS
from pydub import AudioSegment
import speech_recognition as sr
import google.generativeai as genai

from src.text_embedding import retrieve_relevant_chunks
from src.vector_database import load_vector_store

# Set ffmpeg path for PyDub
AudioSegment.converter = r"C:\ffmpeg\bin\ffmpeg.exe"  # <-- update with your ffmpeg path

# ------------------ CONFIG ------------------
st.set_page_config(page_title="ðŸ¤– AI à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤¸à¥‡à¤µà¤¾ à¤šà¥ˆà¤Ÿà¤¬à¥‹à¤Ÿ", layout="wide")

# Load FAISS vector store
index, metadata = load_vector_store()
if index is None:
    st.error("Vector store not found. Run embeddings first.")
    st.stop()

# ------------------ SESSION STATE ------------------
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "user_input" not in st.session_state:
    st.session_state.user_input = ""

# ------------------ HELPER FUNCTIONS ------------------
def record_audio(file_path="user_audio.wav", timeout=10, phrase_time_limit=30):
    recognizer = sr.Recognizer()
    try:
        with sr.Microphone() as source:
            st.info("ðŸŽ¤ à¤µà¤¾à¤¤à¤¾à¤µà¤°à¤£à¤•à¥‹ à¤†à¤µà¤¾à¤œ à¤®à¤¿à¤²à¤¾à¤‰à¤à¤¦à¥ˆ...")
            recognizer.adjust_for_ambient_noise(source, duration=1)
            st.info("ðŸŽ¤ à¤¬à¥‹à¤²à¥à¤¨ à¤¸à¥à¤°à¥ à¤—à¤°à¥à¤¨à¥à¤¹à¥‹à¤¸à¥...")
            audio = recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)
            with open(file_path, "wb") as f:
                f.write(audio.get_wav_data())
            st.success(f"âœ… à¤†à¤µà¤¾à¤œ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤—à¤°à¤¿à¤¯à¥‹: {file_path}")
    except Exception as e:
        st.error(f"âŒ à¤°à¥‡à¤•à¤°à¥à¤¡ à¤—à¤°à¥à¤¨ à¤…à¤¸à¤«à¤²: {e}")

def transcribe_audio(audio_file):
    recognizer = sr.Recognizer()
    try:
        with sr.AudioFile(audio_file) as source:
            audio = recognizer.record(source)
            text = recognizer.recognize_google(audio, language="ne-NP")  # Nepali
        return text
    except Exception as e:
        st.error(f"âŒ à¤Ÿà¥à¤°à¤¾à¤¨à¥à¤¸à¤•à¥à¤°à¤¿à¤ªà¥à¤¸à¤¨ à¤…à¤¸à¤«à¤²: {e}")
        return ""

def speak_text(text, output_file="ai_response.wav"):
    tts = gTTS(text=text, lang="ne", slow=False)
    tts.save("temp.mp3")
    sound = AudioSegment.from_mp3("temp.mp3")
    sound.export(output_file, format="wav")
    
    os_name = platform.system()
    try:
        if os_name == "Windows":
            subprocess.run(["ffplay", "-nodisp", "-autoexit", output_file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        elif os_name == "Darwin":
            subprocess.run(["afplay", output_file])
        elif os_name == "Linux":
            subprocess.run(["ffplay", "-nodisp", "-autoexit", output_file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except Exception as e:
        st.error(f"âŒ à¤…à¤¡à¤¿à¤¯à¥‹ à¤šà¤²à¤¾à¤‰à¤¨ à¤…à¤¸à¤«à¤²: {e}")

def generate_answer_with_rag(question):
    """Use RAG + Gemini to generate Nepali answer."""
    # 1ï¸âƒ£ Retrieve relevant chunks
    results = retrieve_relevant_chunks(question, index, metadata, top_k=3)
    context_texts = [r[0]["text"] for r in results]
    context = "\n\n".join(context_texts)
    
    # 2ï¸âƒ£ Gemini prompt
    prompt = f"à¤¤à¤ªà¤¾à¤ˆà¤‚à¤²à¤¾à¤ˆ à¤¨à¤¿à¤®à¥à¤¨ à¤¸à¤¨à¥à¤¦à¤°à¥à¤­à¤•à¥‹ à¤†à¤§à¤¾à¤°à¤®à¤¾ à¤¨à¥‡à¤ªà¤¾à¤²à¥€à¤®à¤¾ à¤›à¥‹à¤Ÿà¥‹, à¤¸à¤°à¤² à¤° à¤¸à¥à¤ªà¤·à¥à¤Ÿ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¤¿à¤¨à¥à¤¹à¥‹à¤¸à¥:\n\n{context}\n\nà¤ªà¥à¤°à¤¶à¥à¤¨: {question}\n\nà¤‰à¤¤à¥à¤¤à¤°:"
    
    try:
        response = genai.chat.create(
            model="gemini-2.0-turbo",
            messages=[{"author": "user", "content": prompt}]
        )
        return response.last.message.content
    except Exception as e:
        st.error(f"âŒ Gemini failed: {e}")
        return "à¤®à¤¾à¤« à¤—à¤°à¥à¤¨à¥à¤¹à¥‹à¤¸à¥, à¤…à¤¹à¤¿à¤²à¥‡ à¤‰à¤¤à¥à¤¤à¤° à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤›à¥ˆà¤¨à¥¤"

# ------------------ UI ------------------
st.title("ðŸ¤– AI à¤¸à¤°à¤•à¤¾à¤°à¥€ à¤¸à¥‡à¤µà¤¾ à¤šà¥ˆà¤Ÿà¤¬à¥‹à¤Ÿ (à¤¨à¥‡à¤ªà¤¾à¤²à¥€)")
input_mode = st.radio("à¤‡à¤¨à¤ªà¥à¤Ÿ à¤µà¤¿à¤§à¤¿ à¤šà¤¯à¤¨ à¤—à¤°à¥à¤¨à¥à¤¹à¥‹à¤¸à¥", ["âœï¸ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ", "ðŸŽ¤ à¤†à¤µà¤¾à¤œ"])
voice_output = st.checkbox("à¤†à¤µà¤¾à¤œà¤®à¤¾ à¤ªà¥à¤°à¤¤à¤¿à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤¦à¤¿à¤¨à¥à¤¹à¥‹à¤¸à¥", value=True)

# ------------------ GET USER INPUT ------------------
if input_mode == "âœï¸ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ":
    st.session_state.user_input = st.text_input("à¤¤à¤ªà¤¾à¤ˆà¤‚à¤•à¥‹ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤²à¥‡à¤–à¥à¤¨à¥à¤¹à¥‹à¤¸à¥")
else:
    if st.button("ðŸŽ¤ à¤°à¥‡à¤•à¤°à¥à¤¡ à¤—à¤°à¥à¤¨à¥à¤¹à¥‹à¤¸à¥"):
        audio_file = "user_audio.wav"
        record_audio(audio_file)
        st.session_state.user_input = transcribe_audio(audio_file)
        if st.session_state.user_input:
            st.success(f"à¤¤à¤ªà¤¾à¤ˆà¤‚à¤²à¥‡ à¤­à¤¨à¥à¤¨à¥à¤­à¤¯à¥‹: {st.session_state.user_input}")

# ------------------ PROCESS USER INPUT ------------------
if st.button("Send") and st.session_state.user_input.strip() != "":
    user_question = st.session_state.user_input
    response_text = generate_answer_with_rag(user_question)

    if voice_output:
        speak_text(response_text)

    st.session_state.chat_history.append({"user": user_question, "bot": response_text})
    st.session_state.user_input = ""

# ------------------ DISPLAY CHAT HISTORY ------------------
st.subheader("à¤µà¤¾à¤°à¥à¤¤à¤¾à¤²à¤¾à¤ª à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸")
for chat in reversed(st.session_state.chat_history):
    st.markdown(f"**à¤¤à¤ªà¤¾à¤ˆà¤‚:** {chat['user']}")
    st.markdown(f"**à¤¬à¥‹à¤Ÿ:** {chat['bot']}")
    st.markdown("---")
